---
layout: post
title: "Deep Residual Learning for Image Recognition에 대한 지극히 개인적인 이해"
subtitle: "Deep Residual Learning for Image Recognition에 대한 지극히 개인적인 이해"
categories: paper
tags: paper
comments: true
---

처음 퍼블리싱 된지 5년이 다되어가는 지금까지도 Computer Vision 분야에서 Deep한 네트워크를 구성하기 위해서 기본적인 Building Block으로 사용하는 [Deep Residual Learning for Image Recognition](https://arxiv.org/abs/1512.03385/) 에 대하여 간단하게 리뷰하도록 하겠습니다. Residual Architecture의 최초 논문인 해당 논문과 더불어서 Residual Architecture에 대한 나름대로의 해석을 포함한 [Identity Mappings in Deep Residual Networks](https://arxiv.org/abs/1603.05027/), [Residual Networks Behave Like Ensembles of Relatively Shallow Networks](https://arxiv.org/abs/1605.06431/) 이 두 개 논문에 대한 포스팅도 블로그에 올라와있으니, 같이 참고해주시면 Residual Architecture에 대하여 더욱 깊은 이해에 도움이 되실 것이라고 생각합니다.

## Degradation
****
ResNet 논문은 Deep Learning 분야에서 연구를 하는 사람들이라면 당연하게 떠오를 만한 질문에서 시작을 합니다.

`Layer를 단순하게 Stacking 함으로써 Better Learning을 얻을 수 있는가?`

아래 그림은 각 년도 ILSVRC 1등 수상팀이 구현한 Neural Network의 Depth를 보여주는 그림 입니다.

![revolution of depth](/assets/img/20190531/revolution-of-depth.png)
> ResNet이 등장하면서 Neural Network의 Depth가 급격하게 증가하게 됩니다.

그렇다면 왜 ResNet 이전의 Architecture들은 Layer를 깊게 쌓지 못했을까요?
단순하게 생각해보면 Layer를 쌓을수록 Feature Representation을 할 수 있는 Nodes 갯수가 증가하고, 그렇기 때문에 더욱 좋은 성능을 나타낼 것이라고 생각해 볼 수 있습니다.
하지만 아래 그림에는 Layer의 깊이가 특정 임계값을 넘어서면 Performance가 오히려 안좋아지는 것을 보여줍니다.

![degradation](/assets/img/20190531/degradation.png)
> 56-layer에서 Performance가 더 안좋습니다..

저자들은 이처럼 Depth가 깊어짐에 따라서 Accuracy가 Saturated되고, 그 위에 추가적인 Layer를 쌓을수록 Accuracy가 감소하는 문제를 `Degradation`이라고 부릅니다.
`Degradation`은 Overfitting과는 분명하게 다른 문제인데, Overfitting은 Network가 Training Set에 대해서 과도한 Feature Representation을 학습하여 Generlization 능력이 떨어지기 때문에 발생하는 문제로, Training Set에 대해서는 Error가 더 낮아야 합니다.
하지만 `Degradation`은 Training Set에 대해서 조차 Error가 더 높게 발생하는 문제임을 위의 그림에서 확인할 수 있습니다.